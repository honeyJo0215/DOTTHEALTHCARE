# food_text_trainer.py
r"""
í…ìŠ¤íŠ¸ ê¸°ë°˜ ìŒì‹ ë§¤ì¹­ í•™ìŠµ + Vision API í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ëª¨ë“œ 1) í•™ìŠµ (Vision API ì•ˆ ì”€, ë¬´ë£Œ)
    python food_text_trainer.py --train --translate-ko

ëª¨ë“œ 2) í…ŒìŠ¤íŠ¸ (Vision API ì‚¬ìš©, test_images ì•ˆ ì‚¬ì§„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
    python food_text_trainer.py --image test_images\fried_chicken.jpeg --topk 5

ì‚¬ì „ ì¤€ë¹„:
    - food_db.csv ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•¨ (name, serving_size, unit, calories, protein, fat, carbs)
    - Food-101 ë°ì´í„°ëŠ” ì´ì „ì— ì´ë¯¸ì§€ í•™ìŠµ ì½”ë“œì—ì„œ ë‹¤ìš´ë¡œë“œí•´ë‘” ìƒíƒœë¼ê³  ê°€ì •
      (ì—†ìœ¼ë©´ í•œ ë²ˆì€ download=Trueë¡œ Food101ì„ í˜¸ì¶œí•´ì„œ ë°›ì•„ë‘¬ì•¼ í•¨)
"""
import torch
import torch.nn as nn
from torchvision import models, transforms

import argparse
import os
import pickle
from typing import List, Dict

import numpy as np
import pandas as pd

from torchvision.datasets import Food101

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

from google.cloud import vision

try:
    from googletrans import Translator
except ImportError:
    Translator = None


# ---------------- ê²½ë¡œ ì„¤ì • ----------------
FOOD_DB_PATH = "food_db.csv"

VECTORIZER_PATH = "food_text_vectorizer.pkl"
FOOD_VECS_PATH = "food_db_tfidf.npz"
FOOD_META_PATH = "food_db_tfidf_meta.csv"
FOOD101_LABELS_PATH = "food101_labels_ko.csv"

MAX_VISION_LABELS = 10

IMAGE_TRANSFORM = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225]),
])

FOOD101_MODEL_PATH = "food_classifier.pth"
FOOD101_CLASS_NAMES_PATH = "class_names.txt"

_food101_model = None
_food101_class_names = None


def load_food101_model():
    global _food101_model, _food101_class_names
    if _food101_model is not None:
        return _food101_model, _food101_class_names

    if not os.path.exists(FOOD101_MODEL_PATH):
        raise FileNotFoundError(f"{FOOD101_MODEL_PATH} ê°€ ì—†ìŠµë‹ˆë‹¤. Food-101 í•™ìŠµ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    if not os.path.exists(FOOD101_CLASS_NAMES_PATH):
        raise FileNotFoundError(f"{FOOD101_CLASS_NAMES_PATH} ê°€ ì—†ìŠµë‹ˆë‹¤. class_names.txtë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # class_names ë¡œë“œ
    with open(FOOD101_CLASS_NAMES_PATH, "r", encoding="utf-8") as f:
        class_names = [line.strip() for line in f if line.strip()]

    num_classes = len(class_names)

    # EfficientNet-B0 êµ¬ì¡° ì¬êµ¬ì„±
    model = models.efficientnet_b0(weights=None)
    in_features = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(in_features, num_classes)

    state_dict = torch.load(FOOD101_MODEL_PATH, map_location="cpu")
    model.load_state_dict(state_dict)
    model.eval()

    _food101_model = model
    _food101_class_names = class_names
    return _food101_model, _food101_class_names


def predict_food101_labels(image_path: str, topk: int = 3):
    """
    ë¡œì»¬ Food-101 ëª¨ë¸ë¡œ ìƒìœ„ topk í´ë˜ìŠ¤ ì´ë¦„ê³¼ í™•ë¥ ì„ ë°˜í™˜.
    return: (labels, probs)
        labels: ["fried chicken", "chicken wings", ...]
        probs:  [0.85, 0.07, ...]
    """
    model, class_names = load_food101_model()

    from PIL import Image
    img = Image.open(image_path).convert("RGB")
    x = IMAGE_TRANSFORM(img).unsqueeze(0)  # [1, 3, 224, 224]

    with torch.no_grad():
        logits = model(x)
        probs = torch.softmax(logits, dim=1)
        topk_probs, topk_idx = torch.topk(probs, k=min(topk, logits.shape[1]), dim=1)

    labels = []
    probs_list = topk_probs[0].tolist()
    idx_list = topk_idx[0].tolist()

    print("\n[Food-101 ì˜ˆì¸¡ ë¼ë²¨]")
    for rank, (idx, p) in enumerate(zip(idx_list, probs_list), start=1):
        raw_name = class_names[idx]  # ì˜ˆ: "fried_chicken"
        label_str = raw_name.replace("_", " ")
        labels.append(label_str)
        print(f"  {rank}. {label_str} (p={p:.3f})")

    return labels, probs_list

# ---------------- ê³µí†µ ìœ í‹¸ ----------------
def normalize_text(s: str) -> str:
    """ê²€ìƒ‰/ë²¡í„°ë¼ì´ì €ìš© í…ìŠ¤íŠ¸ ì •ê·œí™”"""
    if not isinstance(s, str):
        s = str(s)
    s = s.replace("_", " ")
    return "".join(ch.lower() for ch in s if ch.isalnum() or ch.isspace()).strip()

# ìì£¼ ë‚˜ì˜¤ëŠ” ì“¸ëª¨ì—†ëŠ” ë‹¨ì–´ë“¤(í•„í„°ìš©)
CUSTOM_STOPWORDS = {
    "food", "foods", "recipe", "mix", "service", "company", "brand",
    "browning", "product", "products", "style",
    "vegetable", "vegetables", "fruit", "fruits"
}


def extract_keywords_from_labels(labels, max_keywords: int = 5) -> list[str]:
    """
    Vision ë¼ë²¨ ëª©ë¡ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë§Œ ë½‘ì•„ë‚¸ë‹¤.
    - ì§§ì€ ë‹¨ì–´ ì œê±° (len < 3)
    - stopword ì œê±° (food, mix, service, vegetable ë“±)
    - ì¤‘ë³µ ì œê±°
    """
    # confidence ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    labels_sorted = sorted(labels, key=lambda x: x["score"], reverse=True)
    text = " ".join(l["description"] for l in labels_sorted)
    tokens = normalize_text(text).split()

    keywords: list[str] = []
    for t in tokens:
        if len(t) < 3:
            continue
        if t in CUSTOM_STOPWORDS:
            continue
        if t not in keywords:
            keywords.append(t)

    return keywords[:max_keywords]

def load_food_db(path: str = FOOD_DB_PATH) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"{path} not found. food_dbë¥¼ ë¨¼ì € ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")

    df = pd.read_csv(path)

    need_cols = ["name", "serving_size", "unit", "calories", "protein", "fat", "carbs"]
    for col in need_cols:
        if col not in df.columns:
            if col in ["name", "unit"]:
                df[col] = ""
            else:
                df[col] = 0.0

    df = df[need_cols]
    return df


# ---------------- Vision API ----------------
def get_vision_labels(image_path: str, max_labels: int = MAX_VISION_LABELS) -> List[Dict]:
    """ì´ë¯¸ì§€ì—ì„œ Google Vision APIë¡œ ë¼ë²¨ ëª©ë¡ ì¶”ì¶œ"""
    client = vision.ImageAnnotatorClient()

    with open(image_path, "rb") as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.label_detection(image=image, max_results=max_labels)

    if response.error.message:
        raise RuntimeError(f"Vision API error: {response.error.message}")

    labels = []
    for label in response.label_annotations:
        labels.append({
            "description": label.description,
            "score": label.score,
        })

    print("\n[Vision Labels]")
    for l in labels:
        print(f"- {l['description']} ({l['score']:.3f})")

    return labels


# ---------------- í•™ìŠµ: TF-IDF + Food101 + (ì˜µì…˜) í•œêµ­ì–´ ë²ˆì—­ ----------------
def train_text_matcher(translate_ko: bool = False):
    """
    - food_db.csvì˜ name
    - Food-101 í´ë˜ìŠ¤ ì´ë¦„ (ì˜ì–´ + (ì˜µì…˜) í•œêµ­ì–´ ë²ˆì—­)
    ë¥¼ ì´ìš©í•´ì„œ TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ í•™ìŠµí•˜ê³ ,
    food_db ì´ë¦„ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥.
    """
    print("ğŸ“‚ food_db ë¡œë”© ì¤‘...")
    df_food = load_food_db(FOOD_DB_PATH)

    # 1) Food-101 í´ë˜ìŠ¤ ì´ë¦„ ì½ê¸°
    print("ğŸ“‚ Food-101 í´ë˜ìŠ¤ ì´ë¦„ ë¡œë”© ì¤‘...")
    try:
        food101 = Food101(root="./data", split="train", download=False)
    except RuntimeError:
        # ë°ì´í„°ì…‹ì´ ì—†ìœ¼ë©´ í•œ ë²ˆì€ ì§ì ‘ ë‹¤ìš´ë¡œë“œ í•„ìš”
        print("âš  Food-101 ë°ì´í„°ì…‹ì´ ì—†ì–´ì„œ download=Falseë¡œëŠ” ë¡œë“œ ì‹¤íŒ¨.")
        print("   -> ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ Food101(root='./data', split='train', download=True) í•œ ë²ˆ ëŒë ¤ì£¼ì„¸ìš”.")
        raise

    labels = food101.classes  # ì˜ˆ: ["apple_pie", "bibimbap", ...]
    label_records = []

    translator = None
    if translate_ko:
        if Translator is None:
            print("âš  googletransê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šì•„ ì˜ì–´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            translator = Translator()
            print("ğŸŒ googletrans ì‚¬ìš©: Food-101 í´ë˜ìŠ¤ëª…ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ ì‹œë„")

    corpus: List[str] = []

    # food_dbì˜ ì´ë¦„ë„ ì½”í¼ìŠ¤ì— í¬í•¨
    df_food["name_norm"] = df_food["name"].astype(str).apply(normalize_text)
    corpus.extend(df_food["name_norm"].tolist())

    # Food-101 í´ë˜ìŠ¤ ì´ë¦„ë„ ì½”í¼ìŠ¤ì— ì¶”ê°€ (ì˜ì–´ + í•œêµ­ì–´)
    for label in labels:
        display_en = label.replace("_", " ")
        display_en_norm = normalize_text(display_en)

        display_ko = ""
        if translator is not None:
            try:
                display_ko = translator.translate(display_en, src="en", dest="ko").text
            except Exception as e:
                print(f"  ë²ˆì—­ ì‹¤íŒ¨: {display_en} -> {e}")
                display_ko = ""

        # ì½”í¼ìŠ¤ì— ì¶”ê°€
        corpus.append(display_en_norm)
        if display_ko:
            corpus.append(normalize_text(display_ko))

        label_records.append({
            "label_id": label,
            "display_en": display_en,
            "display_ko": display_ko,
        })

    # Food-101 ë¼ë²¨ â†” í•œê¸€ ë§¤í•‘ íŒŒì¼ ì €ì¥ (UIë‚˜ ë””ë²„ê¹…ì— ìœ ìš©)
    pd.DataFrame(label_records).to_csv(FOOD101_LABELS_PATH, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ Food-101 ë¼ë²¨ ë§¤í•‘ ì €ì¥: {FOOD101_LABELS_PATH}")

    # 2) TF-IDF ë²¡í„°ë¼ì´ì € í•™ìŠµ
    print("ğŸ§  TF-IDF ë²¡í„°ë¼ì´ì € í•™ìŠµ ì¤‘...")
    vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_features=5000)
    vectorizer.fit(corpus)

    with open(VECTORIZER_PATH, "wb") as f:
        pickle.dump(vectorizer, f)
    print(f"ğŸ’¾ ë²¡í„°ë¼ì´ì € ì €ì¥: {VECTORIZER_PATH}")

    # 3) food_db ì´ë¦„ë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜í•´ì„œ ì €ì¥ (í¬ì†Œí–‰ë ¬ .npz)
    print("ğŸ”¢ food_db ì´ë¦„ TF-IDF ë²¡í„°í™” ì¤‘...")
    food_name_vecs = vectorizer.transform(df_food["name_norm"].tolist())
    sparse.save_npz(FOOD_VECS_PATH, food_name_vecs)
    print(f"ğŸ’¾ food_db TF-IDF ë²¡í„° ì €ì¥: {FOOD_VECS_PATH}")

    # 4) food_db ë©”íƒ€(ì˜ì–‘ì •ë³´)ë¥¼ ë³„ë„ ì €ì¥ (ì¸ë±ìŠ¤ ë§ì¶° ì‚¬ìš©)
    df_food[["name", "name_norm", "serving_size", "unit", "calories", "protein", "fat", "carbs"]].to_csv(
        FOOD_META_PATH, index=False, encoding="utf-8-sig"
    )

    print(f"ğŸ’¾ food_db ë©”íƒ€ ì €ì¥: {FOOD_META_PATH}")

    print("âœ… í…ìŠ¤íŠ¸ ë§¤ì¹­ í•™ìŠµ ì™„ë£Œ!")


# ---------------- í…ŒìŠ¤íŠ¸: Vision + TF-IDF ë§¤ì¹­ ----------------
def load_text_matcher():
    if not (os.path.exists(VECTORIZER_PATH) and os.path.exists(FOOD_VECS_PATH) and os.path.exists(FOOD_META_PATH)):
        raise FileNotFoundError(
            "í…ìŠ¤íŠ¸ ë§¤ì¹­ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”:\n"
            "    python food_text_trainer.py --train"
        )

    with open(VECTORIZER_PATH, "rb") as f:
        vectorizer = pickle.load(f)

    food_vecs = sparse.load_npz(FOOD_VECS_PATH)
    df_meta = pd.read_csv(FOOD_META_PATH)

    return df_meta, food_vecs, vectorizer


def predict_from_image(image_path: str, topk: int = 5):
    print(f"ğŸ“· í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {image_path}")
    if not os.path.exists(image_path):
        print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return

    # 0) (ì„ íƒ) Vision ë¼ë²¨ì€ í•­ìƒ ë°›ë˜, ë‚˜ì¤‘ì— fallback ìš©ìœ¼ë¡œë„ ì‚¬ìš©
    try:
        vision_labels = get_vision_labels(image_path)
    except Exception as e:
        print(f"âš  Vision API í˜¸ì¶œ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")
        vision_labels = []

    # 1) Food-101 ë¶„ë¥˜ê¸°ë¡œ dish ì´ë¦„ ì˜ˆì¸¡
    food101_labels, food101_probs = predict_food101_labels(image_path, topk=3)
    max_prob = food101_probs[0] if food101_probs else 0.0

    # 2) í…ìŠ¤íŠ¸ ë§¤ì¹­ ëª¨ë¸ ë¡œë“œ
    print("ğŸ“‚ í…ìŠ¤íŠ¸ ë§¤ì¹­ ëª¨ë¸ ë¡œë”© ì¤‘...")
    df_meta, food_vecs, vectorizer = load_text_matcher()
    if "name_norm" not in df_meta.columns:
        df_meta["name_norm"] = df_meta["name"].astype(str).apply(normalize_text)

    # 3) ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œ ê²°ì • ë¡œì§
    USE_FOOD101_THRESHOLD = 0.5  # ì‹ ë¢°ë„ ê¸°ì¤€

    if max_prob >= USE_FOOD101_THRESHOLD:
        # Food-101 ì˜ˆì¸¡ì´ ê·¸ë‚˜ë§ˆ ë¯¿ì„ ë§Œí•˜ë©´ ì´ê±¸ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©
        print(f"\n[INFO] Food-101 ì˜ˆì¸¡ ì‹ ë¢°ë„ {max_prob:.3f} >= {USE_FOOD101_THRESHOLD}, Food-101 ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©")
        query_text = " ".join(food101_labels)
        fake_label_objs = [{"description": t, "score": 1.0} for t in food101_labels]
        keywords = extract_keywords_from_labels(fake_label_objs, max_keywords=5)
    else:
        # Food-101ê°€ ìì‹  ì—†ìœ¼ë©´ Vision ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ fallback
        print(f"\n[INFO] Food-101 ì˜ˆì¸¡ ì‹ ë¢°ë„ {max_prob:.3f} < {USE_FOOD101_THRESHOLD}, Vision ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ fallback")
        # Vision ë¼ë²¨ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        labels_sorted = sorted(vision_labels, key=lambda x: x["score"], reverse=True)
        query_text = " ".join(l["description"] for l in labels_sorted)
        keywords = extract_keywords_from_labels(vision_labels, max_keywords=5)

    query_norm = normalize_text(query_text)
    print(f"\n[ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸]\n  {query_text}")
    print(f"[ì •ê·œí™” í…ìŠ¤íŠ¸]\n  {query_norm}")
    print(f"[ì¶”ì¶œëœ í‚¤ì›Œë“œ] {keywords}")

    # 4) í‚¤ì›Œë“œê°€ ë“¤ì–´ìˆëŠ” food_db í›„ë³´ë§Œ ìš°ì„  ê²€ìƒ‰
    if keywords:
        mask = df_meta["name_norm"].fillna("").apply(
            lambda s: any(k in s for k in keywords)
        )
        df_cand = df_meta[mask].copy()
        food_vecs_cand = food_vecs[mask.values]
        if len(df_cand) == 0:
            print("âš  í‚¤ì›Œë“œë¡œ ë§¤ì¹­ë˜ëŠ” ìŒì‹ì´ ì—†ì–´ ì „ì²´ food_dbì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            df_cand = df_meta.copy()
            food_vecs_cand = food_vecs
    else:
        df_cand = df_meta.copy()
        food_vecs_cand = food_vecs

    # 5) ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    q_vec = vectorizer.transform([query_norm])
    sims = cosine_similarity(q_vec, food_vecs_cand).flatten()

    if topk > len(sims):
        topk = len(sims)
    top_idx = np.argpartition(-sims, range(topk))[:topk]
    top_idx = top_idx[np.argsort(-sims[top_idx])]

    df_top = df_cand.iloc[top_idx].copy()
    df_top["match_score"] = sims[top_idx]

    print("\nğŸ½ ì¶”ì²œ ìŒì‹ í›„ë³´ (ìƒìœ„ {}ê°œ):".format(topk))
    for i, row in enumerate(df_top.itertuples(), start=1):
        print(f"\n[{i}] {row.name}  (ìœ ì‚¬ë„: {row.match_score:.3f})")
        print(f"    - ê¸°ì¤€ëŸ‰: {row.serving_size} {row.unit}")
        print(f"    - ì¹¼ë¡œë¦¬: {row.calories} kcal")
        print(f"    - íƒ„ìˆ˜í™”ë¬¼: {row.carbs} g")
        print(f"    - ë‹¨ë°±ì§ˆ: {row.protein} g")
        print(f"    - ì§€ë°©: {row.fat} g")


# ---------------- main ----------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train", action="store_true", help="í…ìŠ¤íŠ¸ ë§¤ì¹­ í•™ìŠµ ëª¨ë“œ (Vision API ì‚¬ìš© ì•ˆ í•¨)")
    parser.add_argument("--translate-ko", action="store_true", help="í•™ìŠµ ì‹œ Food-101 ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ ì½”í¼ìŠ¤ì— í¬í•¨")
    parser.add_argument("--image", type=str, help="Vision API + í…ìŠ¤íŠ¸ ë§¤ì¹­ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--topk", type=int, default=5, help="ìƒìœ„ ëª‡ ê°œ ìŒì‹ í›„ë³´ë¥¼ ë³´ì—¬ì¤„ì§€")

    args = parser.parse_args()

    if args.train:
        train_text_matcher(translate_ko=args.translate_ko)
    elif args.image:
        predict_from_image(args.image, topk=args.topk)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
