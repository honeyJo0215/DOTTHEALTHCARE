# server.py
import os
import io
import json
import re
import requests   # â¬… ìƒˆë¡œ ì¶”ê°€
from typing import List, Optional, Dict, Any, Tuple

from fastapi import FastAPI, File, UploadFile, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from torchvision.datasets import Food101
from PIL import Image
import pandas as pd
import numpy as np

from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse
import pickle

# ë²ˆì—­ / Vision (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìë™ ë¹„í™œì„±)
try:
    from googletrans import Translator
except ImportError:
    Translator = None

try:
    from google.cloud import vision
except ImportError:
    vision = None

# ---------------- ê¸°ë³¸ ì„¤ì • ----------------
# ---- ì˜¨ë¼ì¸ ì´ë¯¸ì§€ ê²€ìƒ‰ (Bing Image Search API ì‚¬ìš© ì˜ˆì‹œ) ----
BING_IMAGE_SEARCH_KEY = os.environ.get("BING_IMAGE_SEARCH_KEY")  # Azureì—ì„œ ë°œê¸‰ë°›ì€ í‚¤
BING_IMAGE_SEARCH_ENDPOINT = "https://api.bing.microsoft.com/v7.0/images/search"

if not BING_IMAGE_SEARCH_KEY:
    print("[WARN] BING_IMAGE_SEARCH_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. auto_collect_subdata ëŠ” ì‹¤ì œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œë¥¼ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


MODEL_PATH = "food_classifier.pth"
CLASS_NAMES_PATH = "class_names.txt"
CUSTOM_DATA_DIR = "./custom_data"   # ë¶€ëª¨ ë¼ë²¨ìš© í”¼ë“œë°± ì´ë¯¸ì§€
FOOD_DB_PATH = "food_db.csv"        # name, serving_size, unit, calories, protein, fat, carbs

# í…ìŠ¤íŠ¸ ë§¤ì¹­ìš© (food_text_trainer.py --train ìœ¼ë¡œ ìƒì„±)
VECTORIZER_PATH = "food_text_vectorizer.pkl"
FOOD_VECS_PATH = "food_db_tfidf.npz"
FOOD_META_PATH = "food_db_tfidf_meta.csv"

# í”¼ë“œë°±ì—ì„œ ìŒ“ì´ëŠ” synonym ì €ì¥ íŒŒì¼
FEEDBACK_SYNONYMS_PATH = "feedback_synonyms.json"

# â–¶ ìƒˆë¡œìš´: ë¶€ëª¨ë³„ ì„œë¸Œí´ë˜ìŠ¤ ê´€ë¦¬
SUB_DATA_DIR = "./sub_data"               # sub_data/<parent>/<child>/*.jpg
SUBCLASS_META_PATH = "subclass_meta.json" # ë¶€ëª¨ë³„ children ëª©ë¡
SUBHEAD_DIR = "./subheads"               # ë¶€ëª¨ë³„ ì„œë¸Œí—¤ë“œ weight ì €ì¥

DEVICE = torch.device("cpu")  # í•„ìš”í•˜ë©´ cuda ë¡œ ë³€ê²½ ê°€ëŠ¥

BATCH_SIZE = 16
FINETUNE_EPOCHS = 3
FINETUNE_LR = 1e-4

# ì„œë¸Œí—¤ë“œ í•™ìŠµìš©
SUBHEAD_EPOCHS = 5
SUBHEAD_LR = 1e-3

AUTO_FINETUNE_MIN_SAMPLES = 50  # í”¼ë“œë°± ì´ë¯¸ì§€ê°€ ì´ ì´ìƒ ìŒ“ì´ë©´ íŒŒì¸íŠœë‹ ì¶”ì²œ

app = FastAPI()

# CORS: iOS ì•±ì—ì„œ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ ì—´ì–´ë‘ê¸° (ê°œë°œìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ì„œë¹„ìŠ¤ ì‹œ ë„ë©”ì¸ ì œí•œ ê¶Œì¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------- í…ìŠ¤íŠ¸ ìœ í‹¸ / ë²ˆì—­ / ë§¤ì¹­ ----------------

def normalize_text(s: str) -> str:
    """ê²€ìƒ‰/ë²¡í„°ë¼ì´ì €ìš© í…ìŠ¤íŠ¸ ì •ê·œí™” (ì†Œë¬¸ì + ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ)"""
    if not isinstance(s, str):
        s = str(s)
    s = s.replace("_", " ")
    return "".join(ch.lower() for ch in s if ch.isalnum() or ch.isspace()).strip()


_text_matcher_loaded = False
_text_vectorizer = None
_food_vecs = None
_food_meta_df: Optional[pd.DataFrame] = None
_translator: Optional[Translator] = None


def ensure_translator():
    """googletrans Translator ì´ˆê¸°í™” (ìˆì„ ë•Œë§Œ)"""
    global _translator
    if _translator is None and Translator is not None:
        try:
            _translator = Translator()
            print("[INFO] googletrans Translator ì´ˆê¸°í™” ì™„ë£Œ.")
        except Exception as e:
            print(f"[WARN] Translator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            _translator = None


_text_matcher_loaded = False
_text_vectorizer = None
_food_vecs = None
_food_meta_df: Optional[pd.DataFrame] = None

def rebuild_text_index_from_food_db():
    """
    í˜„ì¬ food_db.csv ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ
    _food_meta_df, _food_vecs ë¥¼ ë‹¤ì‹œ ë§Œë“ ë‹¤.
    - trainer ëŠ” vectorizer.pkl ë§Œ ë§Œë“¤ë©´ ë˜ê³ ,
      tf-idf ì¸ë±ìŠ¤ëŠ” í•­ìƒ ìµœì‹  food_db ë¡œë¶€í„° ê³„ì‚°ëœë‹¤.
    """
    global _food_meta_df, _food_vecs, _text_vectorizer

    if _text_vectorizer is None:
        return

    df = load_food_db_df(limit=None, offset=0)
    if df.empty:
        _food_meta_df = None
        _food_vecs = None
        return

    df = df.copy()
    # ì •ê·œí™”ëœ ì´ë¦„ ì»¬ëŸ¼
    df["name_norm"] = df["name"].astype(str).apply(normalize_text)
    texts = df["name_norm"].fillna("").tolist()

    # ê¸°ì¡´ vocab/idf ë¥¼ ì‚¬ìš©í•´ì„œ í˜„ì¬ DB ì „ì²´ì— ëŒ€í•´ TF-IDF ë²¡í„° ìƒì„±
    _food_vecs = _text_vectorizer.transform(texts)
    _food_meta_df = df
    print(f"[INFO] í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì¬ë¹Œë“œ ì™„ë£Œ: {len(df)} foods")


def load_text_matcher():
    """
    TF-IDF ë²¡í„°ë¼ì´ì €ë¥¼ ë¡œë“œí•˜ê³ ,
    í˜„ì¬ food_db.csvë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¥¼ ë§Œë“ ë‹¤.
    (ì´ì œ ë” ì´ìƒ FOOD_VECS_PATH, FOOD_META_PATH ëŠ” í•„ìˆ˜ê°€ ì•„ë‹˜)
    """
    global _text_matcher_loaded, _text_vectorizer

    if _text_matcher_loaded:
        return

    if not os.path.exists(VECTORIZER_PATH):
        print("[WARN] VECTORIZER_PATH ê°€ ì—†ì–´ í…ìŠ¤íŠ¸ ë§¤ì¹­ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
        _text_matcher_loaded = True
        return

    with open(VECTORIZER_PATH, "rb") as f:
        _text_vectorizer = pickle.load(f)

    ensure_translator()
    # ğŸ”¥ ì—¬ê¸°ì„œ ë°”ë¡œ í˜„ì¬ food_db ê¸°ì¤€ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ë§Œë“ ë‹¤
    rebuild_text_index_from_food_db()

    _text_matcher_loaded = True
    print("[INFO] í…ìŠ¤íŠ¸ ë§¤ì¹­ ëª¨ë¸ ë¡œë”© + ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")


def has_korean(text: str) -> bool:
    """ë¬¸ìì—´ì— í•œê¸€ì´ í¬í•¨ë˜ì—ˆëŠ”ì§€ ê°„ë‹¨íˆ ê²€ì‚¬"""
    return any("\uac00" <= ch <= "\ud7a3" for ch in text)


def translate_to_en_if_korean(query: str) -> Tuple[str, bool]:
    """
    queryì— í•œê¸€ì´ ìˆìœ¼ë©´ googletransë¡œ ì˜ì–´ë¡œ ë²ˆì—­ ì‹œë„.
    - (ë²ˆì—­ëœ_í…ìŠ¤íŠ¸, ë²ˆì—­í–ˆëŠ”ì§€ ì—¬ë¶€) ë°˜í™˜
    - translator ì—†ê±°ë‚˜ ì‹¤íŒ¨í•˜ë©´ (ì›ë¬¸, False) ë°˜í™˜
    """
    if not has_korean(query):
        return query, False

    ensure_translator()
    if _translator is None:
        return query, False

    try:
        res = _translator.translate(query, src="ko", dest="en")
        return res.text, True
    except Exception as e:
        print(f"[WARN] ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return query, False


STOP_WORDS = {
    "food", "foods", "dish", "dishes", "ingredient", "ingredients",
    "cuisine", "meal", "meals", "recipe", "recipes",
    "deep", "fried", "frying", "cooked", "cooking",
    "vegetable", "vegetables", "meat", "leaf", "leafy", "fast",
    "plate", "dining", "table", "lunch", "dinner"
}


def extract_keywords_from_labels(
    label_objs: List[Dict[str, Any]],
    max_keywords: int = 5
) -> List[str]:
    """
    Vision/í…ìŠ¤íŠ¸ label ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì„ ì¶”ì¶œ.
    label_objs: [{"description": "...", "score": float}, ...]
    """
    keywords: List[str] = []
    if not label_objs:
        return keywords

    sorted_labels = sorted(label_objs, key=lambda x: x.get("score", 0.0), reverse=True)

    for obj in sorted_labels:
        desc = obj.get("description", "")
        norm = normalize_text(desc)
        for w in norm.split():
            if len(w) <= 2:
                continue
            if w in STOP_WORDS:
                continue
            if w not in keywords:
                keywords.append(w)
                if len(keywords) >= max_keywords:
                    return keywords
    return keywords


def search_food_candidates_masked(
    query: str,
    keywords: List[str],
    topk: int = 10
):
    """
    ì¿¼ë¦¬ ë¬¸ìì—´(ì˜ì–´ ê¸°ì¤€)ë¡œ TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•´
    food_dbì—ì„œ ìƒìœ„ topk ìŒì‹ í›„ë³´ë¥¼ ë°˜í™˜.
    keywordsê°€ ì£¼ì–´ì§€ë©´ name_normì— í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ìŒì‹ë§Œ ìš°ì„  í›„ë³´ë¡œ ì‚¬ìš©.
    """
    load_text_matcher()
    if _text_vectorizer is None or _food_vecs is None or _food_meta_df is None:
        return []

    q_norm = normalize_text(query)
    if not q_norm:
        return []

    df = _food_meta_df
    if "name_norm" not in df.columns:
        df["name_norm"] = df["name"].astype(str).apply(normalize_text)

    mask = None
    if keywords:
        series = df["name_norm"].fillna("")
        mask_arr = series.apply(lambda s: any(k in s for k in keywords)).to_numpy()
        if mask_arr.any():
            mask = mask_arr

    if mask is not None:
        df_cand = df[mask].copy()
        food_vecs_cand = _food_vecs[mask]
    else:
        df_cand = df
        food_vecs_cand = _food_vecs

    q_vec = _text_vectorizer.transform([q_norm])
    sims = cosine_similarity(q_vec, food_vecs_cand).flatten()

    topk = min(topk, len(sims))
    if topk <= 0:
        return []

    top_idx = np.argpartition(-sims, range(topk))[:topk]
    top_idx = top_idx[np.argsort(-sims[top_idx])]

    df_top = df_cand.iloc[top_idx].copy()
    df_top["match_score"] = sims[top_idx]

    candidates = []
    for row in df_top.itertuples():
        candidates.append({
            "name": row.name,
            "serving_size": float(row.serving_size),
            "unit": row.unit,
            "calories": float(row.calories),
            "protein": float(row.protein),
            "fat": float(row.fat),
            "carbs": float(row.carbs),
            "match_score": float(row.match_score),
        })
    return candidates

# ---------------- Google Vision API ----------------

_vision_client = None

def get_vision_client():
    global _vision_client
    if vision is None:
        return None
    if _vision_client is None:
        _vision_client = vision.ImageAnnotatorClient()
    return _vision_client


def get_vision_labels_from_bytes(
    image_bytes: bytes,
    max_results: int = 10
) -> List[Dict[str, Any]]:
    """
    raw ì´ë¯¸ì§€ bytesë¡œë¶€í„° Vision Label Detection ìˆ˜í–‰.
    """
    client = get_vision_client()
    if client is None:
        print("[WARN] google.cloud.vision ì‚¬ìš© ë¶ˆê°€ (ë¯¸ì„¤ì¹˜ ë˜ëŠ” ì¸ì¦ ë¬¸ì œ).")
        return []

    image = vision.Image(content=image_bytes)
    response = client.label_detection(image=image, max_results=max_results)
    if response.error.message:
        print(f"[WARN] Vision API ì—ëŸ¬: {response.error.message}")
        return []

    labels = []
    for l in response.label_annotations:
        labels.append({"description": l.description, "score": float(l.score)})
    return labels

# ---------------- Food-101 í´ë˜ìŠ¤ ì´ë¦„ ì²˜ë¦¬ ----------------

def get_food101_class_names():
    ds = Food101(root="./data", split="train", download=False)
    return list(ds.classes)


def ensure_class_names():
    """class_names.txtê°€ ì´ìƒí•˜ë©´ Food-101 ê¸°ì¤€ìœ¼ë¡œ ì¬ìƒì„±."""
    if os.path.exists(CLASS_NAMES_PATH):
        with open(CLASS_NAMES_PATH, "r", encoding="utf-8") as f:
            names = [line.strip() for line in f.readlines() if line.strip()]
        if len(names) == 101:
            print(f"[INFO] ê¸°ì¡´ class_names.txt ì‚¬ìš© (í´ë˜ìŠ¤ ìˆ˜: {len(names)})")
            return names

        print(f"[WARN] class_names.txt í´ë˜ìŠ¤ ìˆ˜ ì´ìƒ({len(names)}). ì¬ìƒì„± ì‹œë„.")

    names = get_food101_class_names()
    print(f"[INFO] Food-101ì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ {len(names)}ê°œ ë¡œë“œ")

    with open(CLASS_NAMES_PATH, "w", encoding="utf-8") as f:
        for n in names:
            f.write(n + "\n")
    print("[INFO] class_names.txt ì¬ìƒì„± ì™„ë£Œ")
    return names


class_names: List[str] = ensure_class_names()
class_to_idx = {name: i for i, name in enumerate(class_names)}
NUM_CLASSES = len(class_names)
class_names_norm = [normalize_text(n) for n in class_names]

# ---------------- synonym ë¡œë”©/ì €ì¥ ----------------

_synonyms_cache: Optional[Dict[str, List[str]]] = None

def load_synonyms() -> Dict[str, List[str]]:
    global _synonyms_cache
    if _synonyms_cache is not None:
        return _synonyms_cache
    if os.path.exists(FEEDBACK_SYNONYMS_PATH):
        with open(FEEDBACK_SYNONYMS_PATH, "r", encoding="utf-8") as f:
            _synonyms_cache = json.load(f)
    else:
        _synonyms_cache = {}
    return _synonyms_cache


def save_synonyms(syn: Dict[str, List[str]]):
    global _synonyms_cache
    _synonyms_cache = syn
    with open(FEEDBACK_SYNONYMS_PATH, "w", encoding="utf-8") as f:
        json.dump(syn, f, ensure_ascii=False, indent=2)


def add_synonyms_for_label(label: str, texts: List[str]):
    syn = load_synonyms()
    if label not in syn:
        syn[label] = []
    for t in texts:
        t = t.strip()
        if t and t not in syn[label]:
            syn[label].append(t)
    save_synonyms(syn)

# ---------------- ì„œë¸Œí´ë˜ìŠ¤ ë©”íƒ€/í—¤ë“œ ê´€ë¦¬ ----------------

_subclass_meta: Optional[Dict[str, Any]] = None
_subhead_cache: Dict[str, nn.Module] = {}


def load_subclass_meta() -> Dict[str, Any]:
    global _subclass_meta
    if _subclass_meta is not None:
        return _subclass_meta
    if os.path.exists(SUBCLASS_META_PATH):
        with open(SUBCLASS_META_PATH, "r", encoding="utf-8") as f:
            _subclass_meta = json.load(f)
    else:
        _subclass_meta = {}
    return _subclass_meta


def save_subclass_meta(meta: Dict[str, Any]):
    global _subclass_meta
    _subclass_meta = meta
    with open(SUBCLASS_META_PATH, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)


def register_subclass_label(parent_label: str, child_id: str, child_display: str):
    """
    parent_label ì•„ë˜ì— child_id(ì˜ˆ: green_onion_chicken)ë¥¼ ì„œë¸Œí´ë˜ìŠ¤ë¡œ ë“±ë¡.
    - parent ìì²´ë„ children ë¦¬ìŠ¤íŠ¸ì— í•­ìƒ í¬í•¨ë˜ë„ë¡ ë³´ì¥.
    """
    meta = load_subclass_meta()
    if parent_label not in meta:
        meta[parent_label] = {"children": []}

    children = meta[parent_label]["children"]

    # 1) ë¶€ëª¨ ë¼ë²¨ì„ childrenì— ë„£ì–´ë‘ê¸°
    parent_exists = any(c["id"] == parent_label for c in children)
    if not parent_exists:
        children.append({
            "id": parent_label,
            "display": parent_label.replace("_", " ")
        })

    # 2) ìƒˆ child ì¶”ê°€
    exists = any(c["id"] == child_id for c in children)
    if not exists:
        children.append({"id": child_id, "display": child_display})

    meta[parent_label]["children"] = children
    save_subclass_meta(meta)
    print(f"[INFO] ì„œë¸Œí´ë˜ìŠ¤ ë“±ë¡: parent={parent_label}, child={child_id} ({child_display})")


def extract_subclass_id(en_text: str) -> Tuple[str, str]:
    """
    ì˜ì–´ í…ìŠ¤íŠ¸(en_text)ë¡œë¶€í„° ì„œë¸Œí´ë˜ìŠ¤ idì™€ display ìƒì„±.
    - id: green_onion_chicken
    - display: green onion chicken
    """
    norm = normalize_text(en_text)  # "green onion chicken"
    if not norm:
        return "custom_food", en_text.strip() or "custom food"
    sub_id = norm.replace(" ", "_")  # "green_onion_chicken"
    display = norm  # ê·¸ëŒ€ë¡œ display ì‚¬ìš©
    return sub_id, display


def load_subhead(parent_label: str) -> Optional[nn.Module]:
    """
    parent_labelì— ëŒ€í•œ ì„œë¸Œí—¤ë“œ Linear(in=1280, out=n_children) ë¡œë“œ.
    ì—†ìœ¼ë©´ None.
    """
    if parent_label in _subhead_cache:
        return _subhead_cache[parent_label]

    meta = load_subclass_meta()
    if parent_label not in meta:
        return None

    children = meta[parent_label].get("children", [])
    if len(children) < 2:
        return None

    head_path = os.path.join(SUBHEAD_DIR, f"{parent_label}_head.pth")
    if not os.path.exists(head_path):
        return None

    # head í¬ê¸°ëŠ” children ìˆ˜ì— ë”°ë¼ ê²°ì •
    out_dim = len(children)
    head = nn.Linear(1280, out_dim)
    state_dict = torch.load(head_path, map_location=DEVICE)
    head.load_state_dict(state_dict)
    head.to(DEVICE)
    head.eval()

    _subhead_cache[parent_label] = head
    print(f"[INFO] ì„œë¸Œí—¤ë“œ ë¡œë“œ ì™„ë£Œ: {parent_label}, subclasses={out_dim}")
    return head

# ---------------- í‚¤ì›Œë“œ â†’ Food-101 ë¶€ëª¨ ë¼ë²¨ ë§¤í•‘ ----------------

def map_keywords_to_food101_label(
    keywords: List[str]
) -> Tuple[Optional[str], Dict[str, str]]:
    """
    ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ ë°›ì•„ì„œ, ê·¸ ì¤‘ì—ì„œ Food-101 ë¼ë²¨ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒì„ ì°¾ëŠ”ë‹¤.
    - ë°˜í™˜: (ë¶€ëª¨ Food-101 ë¼ë²¨, {ì›ë˜í‚¤ì›Œë“œ: ì˜ì–´ë²ˆì—­í…ìŠ¤íŠ¸})
    """
    load_text_matcher()  # ë²ˆì—­ê¸° ì„¸íŒ… ìœ„í•´

    keyword_en_map: Dict[str, str] = {}
    best_label = None
    best_score = 0

    for kw in keywords:
        raw = kw.strip()
        if not raw:
            continue

        en, translated = translate_to_en_if_korean(raw)
        if translated:
            print(f"[INFO] í‚¤ì›Œë“œ ë²ˆì—­: '{raw}' -> '{en}'")
        keyword_en_map[raw] = en

        norm_en = normalize_text(en)
        if not norm_en:
            continue

        kw_tokens = set(norm_en.split())
        for cls_name, cls_norm in zip(class_names, class_names_norm):
            cls_tokens = set(cls_norm.split())

            overlap = len(kw_tokens & cls_tokens)
            score = overlap

            if cls_norm in norm_en or norm_en in cls_norm:
                score += 1

            if score > best_score:
                best_score = score
                best_label = cls_name

    if best_label:
        print(f"[INFO] í‚¤ì›Œë“œ {keywords} -> Food-101 ë¼ë²¨ ë§¤í•‘ ê²°ê³¼: {best_label} (score={best_score})")
    else:
        print(f"[WARN] í‚¤ì›Œë“œ {keywords} ë¡œ Food-101 ë¼ë²¨ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return best_label, keyword_en_map

# ---------------- ëª¨ë¸ ë¡œë“œ ë° feature ì¶”ì¶œ ----------------

def load_model():
    if not os.path.exists(MODEL_PATH):
        raise RuntimeError(f"{MODEL_PATH} ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ëŒë ¤ì£¼ì„¸ìš”.")

    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)
    num_classes_ckpt = state_dict["classifier.1.weight"].shape[0]
    print(f"[INFO] ì²´í¬í¬ì¸íŠ¸ ê¸°ì¤€ í´ë˜ìŠ¤ ìˆ˜: {num_classes_ckpt}")

    if num_classes_ckpt != NUM_CLASSES:
        print(f"[WARN] checkpoint({num_classes_ckpt}) vs class_names({NUM_CLASSES}) ë¶ˆì¼ì¹˜.")

    model = models.efficientnet_b0(weights=None)
    in_features = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(in_features, num_classes_ckpt)
    model.load_state_dict(state_dict)
    model.to(DEVICE)
    model.eval()
    return model


model = load_model()


def extract_features(x: torch.Tensor) -> torch.Tensor:
    """
    EfficientNet ì¤‘ê°„ feature ì¶”ì¶œ (classifier ì• 1280ì°¨ì› ë²¡í„°)
    - í•­ìƒ eval ëª¨ë“œì—ì„œ dropout/bn ê³ ì •
    """
    model.eval()
    with torch.no_grad():
        feats = model.features(x)
        feats = model.avgpool(feats)
        feats = torch.flatten(feats, 1)
    return feats


# ---------------- ê³µí†µ Transform ----------------

transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225]),
])

# ---------------- Custom Dataset (ë¶€ëª¨ ë¼ë²¨ìš©) ----------------

class CustomLabeledDataset(Dataset):
    """
    custom_data/<label_name>/*.jpg êµ¬ì¡°ë¥¼ ì½ì–´ì„œ
    ìš°ë¦¬ class_to_idx ê¸°ì¤€ label indexë¡œ ë°˜í™˜.
    class_to_idxì— ì—†ëŠ” ë¼ë²¨ í´ë”ëŠ” ê·¸ëƒ¥ ê±´ë„ˆëœ€.
    """
    def __init__(self, root, transform, class_to_idx):
        self.samples = []
        self.transform = transform
        self.class_to_idx = class_to_idx

        if not os.path.exists(root):
            return

        for label_name in os.listdir(root):
            label_dir = os.path.join(root, label_name)
            if not os.path.isdir(label_dir):
                continue
            if label_name not in class_to_idx:
                print(f"[WARN] custom_dataì— '{label_name}' í´ë”ê°€ ìˆì§€ë§Œ class_to_idxì— ì—†ìŒ -> í•™ìŠµì—ì„œ ì œì™¸")
                continue
            label_idx = class_to_idx[label_name]
            for fname in os.listdir(label_dir):
                if fname.lower().endswith((".jpg", ".jpeg", ".png", ".bmp")):
                    path = os.path.join(label_dir, fname)
                    self.samples.append((path, label_idx))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        img = Image.open(path).convert("RGB")
        img = self.transform(img)
        return img, label

# ---------------- ì„œë¸Œí´ë˜ìŠ¤ìš© Dataset ----------------

class SubclassDataset(Dataset):
    """
    sub_data/<parent>/<child>/*.jpg êµ¬ì¡°ë¥¼ ì½ì–´ì„œ
    parent_labelì— ëŒ€í•œ child index(0..n_child-1)ë¥¼ ë°˜í™˜.
    """
    def __init__(self, parent_label: str, child_ids: List[str], transform):
        self.samples = []
        self.transform = transform
        self.child_ids = child_ids
        root = os.path.join(SUB_DATA_DIR, parent_label)
        if not os.path.exists(root):
            return

        for ci, cid in enumerate(child_ids):
            child_dir = os.path.join(root, cid)
            if not os.path.isdir(child_dir):
                continue
            for fname in os.listdir(child_dir):
                if not fname.lower().endswith((".jpg", ".jpeg", ".png", ".bmp")):
                    continue
                path = os.path.join(child_dir, fname)
                self.samples.append((path, ci))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, ci = self.samples[idx]
        img = Image.open(path).convert("RGB")
        img = self.transform(img)
        return img, ci

# ---------------- ìœ í‹¸ í•¨ìˆ˜ ----------------

def slugify_label(name: str) -> str:
    """
    ìŒì‹ ì´ë¦„ì„ íŒŒì¼/í´ë” ì´ë¦„ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ìŠ¬ëŸ¬ê·¸ë¡œ ë³€í™˜.
    - ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” '_' ë¡œ ì¹˜í™˜.
    - ì „ë¶€ ë‚ ì•„ê°€ë©´ 'food' ë¡œ ëŒ€ì²´.
    """
    s = name.strip().lower()
    # í•œê¸€ ë“± ë¹„ASCIIëŠ” ì¼ë‹¨ '_' ë¡œ ì²˜ë¦¬ (í•„ìš”í•˜ë©´ translatorë¡œ ì˜ì–´ ë³€í™˜í•´ì„œ ì“°ë©´ ë¨)
    s = re.sub(r"[^a-z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    if not s:
        s = "food"
    return s

def search_food_images_online(query: str, count: int = 3) -> List[str]:
    """
    Bing Image Search APIë¡œ ìŒì‹ ì´ë¯¸ì§€ URLë“¤ì„ ê°€ì ¸ì˜¨ë‹¤.
    - query: ê²€ìƒ‰ì–´ (ì˜ˆ: 'fried chicken food')
    - count: ê°€ì ¸ì˜¬ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜
    """
    if not BING_IMAGE_SEARCH_KEY:
        print("[WARN] BING_IMAGE_SEARCH_KEY ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []

    headers = {
        "Ocp-Apim-Subscription-Key": BING_IMAGE_SEARCH_KEY
    }
    params = {
        "q": query,
        "count": count,
        "safeSearch": "Strict",
        "imageType": "Photo",
    }

    try:
        resp = requests.get(BING_IMAGE_SEARCH_ENDPOINT, headers=headers, params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        urls = [item["contentUrl"] for item in data.get("value", []) if "contentUrl" in item]
        return urls
    except Exception as e:
        print(f"[WARN] ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: query='{query}', error={e}")
        return []
    
def download_image_from_url(url: str) -> Optional[Image.Image]:
    """
    ì£¼ì–´ì§„ URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ PIL Imageë¡œ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ None.
    """
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        img = Image.open(io.BytesIO(resp.content)).convert("RGB")
        return img
    except Exception as e:
        print(f"[WARN] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: url='{url}', error={e}")
        return None

def predict_parent_label_from_pil(img: Image.Image) -> Optional[str]:
    """
    PIL ì´ë¯¸ì§€ë¥¼ EfficientNet(Food-101 í—¤ë“œ)ë¡œ ì¶”ë¡ í•˜ì—¬
    top-1 ë¶€ëª¨ ë¼ë²¨ì„ ë°˜í™˜.
    """
    model.eval()
    tensor = transform(img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        outputs = model(tensor)
        probs = torch.softmax(outputs, dim=1)[0]
        top_prob, top_idx = torch.max(probs, dim=0)

    idx = top_idx.item()
    if idx < 0 or idx >= len(class_names):
        return None
    return class_names[idx]

def save_sub_image(img: Image.Image, parent_label: str, child_id: str) -> str:
    """
    ì„œë¸Œ ë°ì´í„° ì´ë¯¸ì§€ í•œ ì¥ì„
    sub_data/<parent_label>/<child_id>/<child_id>_N.jpg ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜.
    """
    parent_dir = os.path.join(SUB_DATA_DIR, parent_label)
    child_dir = os.path.join(parent_dir, child_id)
    os.makedirs(child_dir, exist_ok=True)

    existing = [
        f for f in os.listdir(child_dir)
        if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp"))
    ]
    fname = f"{child_id}_{len(existing)}.jpg"
    path = os.path.join(child_dir, fname)
    img.save(path)
    return path


def pil_from_bytes(image_bytes: bytes) -> Image.Image:
    return Image.open(io.BytesIO(image_bytes)).convert("RGB")


def pil_from_upload(file: UploadFile) -> Image.Image:
    contents = file.file.read()
    return pil_from_bytes(contents)


def predict_tensor(tensor: torch.Tensor, topk: int = 3):
    model.eval()
    with torch.no_grad():
        outputs = model(tensor)
        probs = torch.softmax(outputs, dim=1)[0]
        top_probs, top_idxs = torch.topk(probs, k=topk)
    results = []
    for p, idx in zip(top_probs, top_idxs):
        idx = idx.item()
        conf = float(p.item())
        raw_label = class_names[idx] if 0 <= idx < len(class_names) else f"unknown_{idx}"
        display_label = raw_label.replace("_", " ")
        results.append({"label": raw_label, "label_display": display_label, "confidence": conf})
    return results


def count_feedback_samples():
    """
    custom_data í´ë” ë‚´ ì´ë¯¸ì§€ ê°œìˆ˜ì™€ ë¼ë²¨ë³„ ê°œìˆ˜ ì§‘ê³„
    """
    total = 0
    label_counts: Dict[str, int] = {}
    if not os.path.exists(CUSTOM_DATA_DIR):
        return {"total": 0, "labels": {}}

    for label_name in os.listdir(CUSTOM_DATA_DIR):
        label_dir = os.path.join(CUSTOM_DATA_DIR, label_name)
        if not os.path.isdir(label_dir):
            continue
        count = sum(
            1 for f in os.listdir(label_dir)
            if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp"))
        )
        if count > 0:
            label_counts[label_name] = count
            total += count

    return {"total": total, "labels": label_counts}


def list_finetune_items() -> List[Dict[str, Any]]:
    """
    custom_data/ ì•„ë˜ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ í‰íƒ„í™”í•´ì„œ
    [{"id": 0, "label": "...", "filename": "...", "path": "..."}] í˜•íƒœë¡œ ë°˜í™˜.
    """
    items: List[Dict[str, Any]] = []
    idx = 0
    if os.path.exists(CUSTOM_DATA_DIR):
        for label in sorted(os.listdir(CUSTOM_DATA_DIR)):
            label_dir = os.path.join(CUSTOM_DATA_DIR, label)
            if not os.path.isdir(label_dir):
                continue
            for fname in sorted(os.listdir(label_dir)):
                if not fname.lower().endswith((".jpg", ".jpeg", ".png", ".bmp")):
                    continue
                path = os.path.join(label_dir, fname)
                items.append({
                    "id": idx,
                    "label": label,
                    "filename": fname,
                    "path": path,
                })
                idx += 1
    return items

# ---------------- food_db ìœ í‹¸ ----------------

FOOD_DB_COLUMNS = ["name", "serving_size", "unit", "calories", "protein", "fat", "carbs"]


def load_food_db_df(limit: int | None = None, offset: int = 0) -> pd.DataFrame:
    """
    food_db.csvë¥¼ ë©”ëª¨ë¦¬ ì•ˆì „í•˜ê²Œ ì½ê¸° ìœ„í•œ í•¨ìˆ˜.
    """
    if not os.path.exists(FOOD_DB_PATH):
        return pd.DataFrame(columns=FOOD_DB_COLUMNS)

    read_kwargs = {
        "usecols": lambda c: c in FOOD_DB_COLUMNS,
    }

    if limit is not None:
        read_kwargs["nrows"] = offset + limit if offset >= 0 else limit

    df = pd.read_csv(FOOD_DB_PATH, **read_kwargs)

    for col in FOOD_DB_COLUMNS:
        if col not in df.columns:
            df[col] = 0.0 if col not in ["name", "unit"] else ""

    if limit is not None:
        if offset < 0:
            offset = 0
        df = df.iloc[offset: offset + limit]

    return df[FOOD_DB_COLUMNS]


def save_food_db_df(df: pd.DataFrame):
    df.to_csv(FOOD_DB_PATH, index=False)


def prepare_db_name(src_name: str) -> Tuple[str, Optional[str], bool]:
    """
    DBì— ì €ì¥í•  ì´ë¦„ì„ ì¤€ë¹„:
    - í•œê¸€ì´ë©´ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ ì˜ì–´ë¥¼ nameìœ¼ë¡œ ì‚¬ìš©
    - (stored_name, original_name_if_translated_else_None, translated_flag)
    """
    src_name = src_name.strip()
    if not src_name:
        return src_name, None, False

    en, translated = translate_to_en_if_korean(src_name)
    stored = en.strip() if en.strip() else src_name
    return stored, (src_name if translated else None), translated

# ---------------- Pydantic ëª¨ë¸ ----------------

class AutoCollectRequest(BaseModel):
    start_index: int = 0       # food_dbì—ì„œ ì‹œì‘í•  ì¸ë±ìŠ¤
    max_items: int = 10        # ì´ë²ˆ í˜¸ì¶œì—ì„œ ì²˜ë¦¬í•  ìŒì‹ ê°œìˆ˜
    images_per_food: int = 3   # ìŒì‹ í•˜ë‚˜ë‹¹ ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ ìˆ˜
    language: str = "en"       # ê²€ìƒ‰ìš© ì–¸ì–´ (ì§€ê¸ˆì€ 'en'ë§Œ ì‚¬ìš©)


class FoodCreate(BaseModel):
    name: str
    carbs: float     # g
    protein: float   # g
    fat: float       # g
    serving_size: float = 1.0
    unit: str = "serving"
    calories: Optional[float] = None


class FoodSearchRequest(BaseModel):
    query: str
    topk: int = 10

# ---------------- DBì— ìŒì‹ ì¶”ê°€í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜ ----------------

def add_food_row(
    name: str,
    carbs: float,
    protein: float,
    fat: float,
    serving_size: float = 1.0,
    unit: str = "serving",
    calories: Optional[float] = None
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    /foods/addì™€ /feedbackì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë‚´ë¶€ DB ì¶”ê°€ í•¨ìˆ˜.
    - nameì— í•œê¸€ì´ ìˆìœ¼ë©´ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ ì €ì¥.
    """
    df = load_food_db_df()

    stored_name, original_name, translated = prepare_db_name(name)

    name_lower = stored_name.lower()
    if not df.empty:
        exists = df["name"].astype(str).str.lower() == name_lower
        if exists.any():
            raise ValueError(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìŒì‹ì…ë‹ˆë‹¤: {stored_name}")

    if calories is None:
        calories = 4.0 * carbs + 4.0 * protein + 9.0 * fat

    new_row = {
        "name": stored_name,
        "serving_size": float(serving_size),
        "unit": unit,
        "calories": float(calories),
        "protein": float(protein),
        "fat": float(fat),
        "carbs": float(carbs),
    }

    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
    save_food_db_df(df)

    meta = {
        "stored_name": stored_name,
        "original_name": original_name or name,
        "translated": translated,
    }
    return new_row, meta

# ---------------- ì„œë¸Œí—¤ë“œ inference ----------------

def apply_subclass_head(tensor: torch.Tensor, base_pred: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    base_pred ì—ì„œ ë¶€ëª¨ ë¼ë²¨ì„ êº¼ë‚´ê³ , í•´ë‹¹ parentì— ëŒ€í•œ ì„œë¸Œí—¤ë“œê°€ ìˆìœ¼ë©´
    1280ì°¨ì› featureë¥¼ ë„£ì–´ ì„œë¸Œë¼ë²¨ì„ ì˜ˆì¸¡.
    """
    parent_label = base_pred["label"]
    meta = load_subclass_meta()
    if parent_label not in meta:
        return None

    children = meta[parent_label].get("children", [])
    if len(children) < 2:
        return None

    head = load_subhead(parent_label)
    if head is None:
        return None

    head.eval()
    with torch.no_grad():
        feats = extract_features(tensor.to(DEVICE))
        logits = head(feats)
        probs = torch.softmax(logits, dim=1)[0]
        top_prob, top_idx = torch.max(probs, dim=0)

    ci = top_idx.item()
    child = children[ci]
    sub_id = child["id"]
    sub_display = child.get("display", sub_id.replace("_", " "))

    return {
        "parent_label": parent_label,
        "sub_label": sub_id,
        "sub_label_display": sub_display,
        "confidence": float(top_prob.item()),
    }

def apply_best_subhead_any_parent(tensor: torch.Tensor) -> Optional[Dict[str, Any]]:
    """
    ë¶€ëª¨ ì˜ˆì¸¡ì´ í‹€ë ¸ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ fallback:
    - subclass_metaì— ë“±ë¡ëœ ëª¨ë“  parentì— ëŒ€í•´ ì„œë¸Œí—¤ë“œë¥¼ ëŒë ¤ë³´ê³ 
    - ì „ì²´ parent/child ì¤‘ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ì¡°í•©ì„ í•˜ë‚˜ ë°˜í™˜.
    """
    meta = load_subclass_meta()
    if not meta:
        return None

    # ê³µí†µ feature ì¶”ì¶œ (1ë²ˆë§Œ)
    feats = extract_features(tensor.to(DEVICE))

    best = None

    for parent_label, info in meta.items():
        children = info.get("children", [])
        if len(children) < 2:
            continue

        head = load_subhead(parent_label)
        if head is None:
            continue

        head.eval()
        with torch.no_grad():
            logits = head(feats)  # [1, n_child]
            probs = torch.softmax(logits, dim=1)[0]
            top_prob, top_idx = torch.max(probs, dim=0)

        ci = top_idx.item()
        child = children[ci]
        sub_id = child["id"]
        sub_display = child.get("display", sub_id.replace("_", " "))

        conf = float(top_prob.item())

        if (best is None) or (conf > best["confidence"]):
            best = {
                "parent_label": parent_label,
                "sub_label": sub_id,
                "sub_label_display": sub_display,
                "confidence": conf,
                "from_fallback": True,
            }

    return best

# ---------------- API: ì˜ˆì¸¡ ----------------

@app.post("/predict")
async def predict(image: UploadFile = File(...), topk: int = 3, food_topk: int = 5):
    """
    ì´ë¯¸ì§€ â†’ (EfficientNet ë¶€ëª¨ + (ìˆìœ¼ë©´) ì„œë¸Œí—¤ë“œ) â†’ í…ìŠ¤íŠ¸ ë§¤ì¹­(TF-IDF) â†’ food_db í›„ë³´ ë°˜í™˜.
    """
    try:
        image_bytes = await image.read()
        img = pil_from_bytes(image_bytes)

        tensor = transform(img).unsqueeze(0).to(DEVICE)
        predictions = predict_tensor(tensor, topk=topk)
        max_prob = predictions[0]["confidence"] if predictions else 0.0

        # ì„œë¸Œí—¤ë“œê°€ ìˆë‹¤ë©´ ë¶€ëª¨ ë¼ë²¨ì— ëŒ€í•´ ì„œë¸Œë¼ë²¨ ë³´ì •
        sub_prediction = None
        if predictions:
            sub_prediction = apply_subclass_head(tensor, predictions[0])

        # Vision ë¼ë²¨ (í…ìŠ¤íŠ¸ ë³´ì¡°)
        vision_labels = get_vision_labels_from_bytes(image_bytes, max_results=10)
        # 2ì°¨ ì‹œë„(í´ë°±): ë¶€ëª¨ ì˜ˆì¸¡ì´ í‹€ë ¸ê±°ë‚˜ ì„œë¸Œí—¤ë“œê°€ ì—†ëŠ” ê²½ìš°,
        # ëª¨ë“  parentì˜ ì„œë¸Œí—¤ë“œë¥¼ ëŒë ¤ì„œ í•œ ë²ˆì´ë¼ë„ best subclassë¥¼ ì°¾ì•„ë³¸ë‹¤.
        if sub_prediction is None:
            sub_prediction = apply_best_subhead_any_parent(tensor)
        USE_FOOD101_THRESHOLD = 0.5
        SUBLABEL_THRESHOLD = 0.4

        # í…ìŠ¤íŠ¸ ë§¤ì¹­ì— ì‚¬ìš©í•  ë¼ë²¨ í…ìŠ¤íŠ¸ ê²°ì •
        if sub_prediction and sub_prediction["confidence"] >= SUBLABEL_THRESHOLD:
            # ì„œë¸Œë¼ë²¨ì´ ì¶©ë¶„íˆ ìì‹ ìˆì„ ë•Œ ì„œë¸Œë¼ë²¨ í…ìŠ¤íŠ¸ ì‚¬ìš©
            top_label_text = sub_prediction["sub_label_display"]
        else:
            # ì„œë¸Œí—¤ë“œ ì—†ê±°ë‚˜ ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ ë¶€ëª¨ ë¼ë²¨ ì‚¬ìš©
            if predictions and max_prob >= USE_FOOD101_THRESHOLD:
                top_label_text = predictions[0]["label_display"]
            else:
                top_label_text = None

        if top_label_text is not None:
            print(f"[INFO] í…ìŠ¤íŠ¸ ë§¤ì¹­ì— ì‚¬ìš©í•  ë¼ë²¨ í…ìŠ¤íŠ¸: {top_label_text}")
            query_text = top_label_text
            fake_labels = [{"description": top_label_text, "score": max_prob}]
            keywords = extract_keywords_from_labels(fake_labels, max_keywords=5)
        else:
            # Food-101 ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ Vision ë¼ë²¨ë“¤ì˜ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©
            query_text = " ".join(l["description"] for l in vision_labels)
            keywords = extract_keywords_from_labels(vision_labels, max_keywords=5)
            # Vision ë¼ë²¨ë„ ì—†ìœ¼ë©´, ì–´ì©” ìˆ˜ ì—†ì´ top-1 Food-101 ë¼ë²¨ì´ë¼ë„ ì‚¬ìš©
            if not query_text and predictions:
                query_text = predictions[0]["label_display"]
        # ë¶€ëª¨ ë¼ë²¨ì— ê±¸ë¦° synonym ë„ ì¿¼ë¦¬ì— ì¶”ê°€
        synonyms_for_parent: List[str] = []
        if predictions:
            top1_raw = predictions[0]["label"]
            syn = load_synonyms().get(top1_raw, [])
            if syn:
                synonyms_for_parent = syn
                query_text = query_text + " " + " ".join(syn)
                for s in syn:
                    norm = normalize_text(s)
                    if norm and norm not in keywords:
                        keywords.append(norm)

        candidates = search_food_candidates_masked(query_text, keywords, topk=food_topk)

        return JSONResponse({
            "predictions": predictions,
            "sub_prediction": sub_prediction,
            "vision_labels": vision_labels,
            "query_text": query_text,
            "keywords": keywords,
            "synonyms": synonyms_for_parent,
            "candidates": candidates
        })
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)

# ---------------- API: í”¼ë“œë°± (í‚¤ì›Œë“œ + ì„œë¸Œë¼ë²¨ + DB ì¶”ê°€) ----------------

@app.post("/feedback")
async def feedback(
    image: UploadFile = File(...),
    keywords: str = Form(...),         # "ì¹˜í‚¨, íŒŒë‹­, fried chicken"
    main_keyword: Optional[str] = Form(None),  # ì´ ì¤‘ ì„œë¸Œë¼ë²¨ë¡œ ì“¸ í•˜ë‚˜ (ì—†ìœ¼ë©´ ì²« ë²ˆì§¸)
    use_for_training: bool = Form(True),
    add_to_db: bool = Form(False),
    db_name: Optional[str] = Form(None),
    db_carbs: Optional[float] = Form(None),
    db_protein: Optional[float] = Form(None),
    db_fat: Optional[float] = Form(None),
    db_serving_size: float = Form(1.0),
    db_unit: str = Form("serving"),
    db_calories: Optional[float] = Form(None),
):
    """
    ì‚¬ìš©ìê°€ 'ì´ ìŒì‹ì€ Xì•¼' ë¼ê³  ì•Œë ¤ì¤„ ë•Œ í˜¸ì¶œ.

    - keywords: "ì¹˜í‚¨, íŒŒë‹­, fried chicken" ì²˜ëŸ¼ ì—¬ëŸ¬ í‚¤ì›Œë“œ ì…ë ¥ ê°€ëŠ¥.
    - main_keyword: ê·¸ ì¤‘ ì„œë¸Œë¼ë²¨ë¡œ ì‚¬ìš©í•  í•˜ë‚˜ (ì˜ˆ: "íŒŒë‹­")
    - use_for_training:
        True  -> ë¶€ëª¨ Food-101 ë¼ë²¨ ë§¤í•‘ + custom_data + sub_dataì— ì´ë¯¸ì§€ ì €ì¥ + synonym ì ë¦½
        False -> í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì„ íƒì ìœ¼ë¡œ add_to_dbë§Œ ìˆ˜í–‰)
    - add_to_db:
        True ì´ê³  macrosê°€ ì£¼ì–´ì§€ë©´ food_dbì— ìƒˆ ìŒì‹ìœ¼ë¡œ ì¶”ê°€
    """
    # 0) í‚¤ì›Œë“œ íŒŒì‹±
    raw_keywords = [k.strip() for k in keywords.replace(";", ",").split(",")]
    raw_keywords = [k for k in raw_keywords if k]

    if not raw_keywords:
        return JSONResponse({"error": "keywords ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ˆ: 'ì¹˜í‚¨, íŒŒë‹­, fried chicken'"}, status_code=400)

    if main_keyword is None or main_keyword.strip() not in raw_keywords:
        main_keyword = raw_keywords[0]
    main_keyword = main_keyword.strip()

    # 1) DB ì¶”ê°€ (ì˜µì…˜)
    db_result = None
    db_error = None
    if add_to_db:
        if db_name is None or db_carbs is None or db_protein is None or db_fat is None:
            db_error = "add_to_db=True ì¸ ê²½ìš° db_name, db_carbs, db_protein, db_fat ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            try:
                new_row, meta = add_food_row(
                    name=db_name,
                    carbs=db_carbs,
                    protein=db_protein,
                    fat=db_fat,
                    serving_size=db_serving_size,
                    unit=db_unit,
                    calories=db_calories,
                )
                db_result = {
                    "food": new_row,
                    "meta": meta,
                }
            except ValueError as ve:
                db_error = str(ve)
            except Exception as e:
                db_error = f"DB ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}"

    # 2) í•™ìŠµìš© í”¼ë“œë°± (ë¶€ëª¨ + ì„œë¸Œë¼ë²¨)
    mapped_label = None
    keyword_en_map: Dict[str, str] = {}
    saved_path = None
    is_known = False
    sub_label_id = None
    sub_label_display = None

    if use_for_training:
        mapped_label, keyword_en_map = map_keywords_to_food101_label(raw_keywords)

        img = pil_from_upload(image)

        if not mapped_label:
            # ë¶€ëª¨ ë¼ë²¨ ëª»ì°¾ìœ¼ë©´ unmapped í´ë”ì—ë§Œ ì €ì¥
            os.makedirs(CUSTOM_DATA_DIR, exist_ok=True)
            unknown_dir = os.path.join(CUSTOM_DATA_DIR, "unmapped")
            os.makedirs(unknown_dir, exist_ok=True)

            existing = [
                f for f in os.listdir(unknown_dir)
                if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp"))
            ]
            fname = f"unmapped_{len(existing)}.jpg"
            saved_path = os.path.join(unknown_dir, fname)
            img.save(saved_path)

            stats = count_feedback_samples()
            can_finetune = stats["total"] >= AUTO_FINETUNE_MIN_SAMPLES

            return JSONResponse(
                {
                    "message": "Food-101 ë¶€ëª¨ ë¼ë²¨ë¡œ ë§¤í•‘í•˜ì§€ ëª»í•´ unmapped í´ë”ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.",
                    "saved_path": saved_path,
                    "mapped_label": None,
                    "is_known_class": False,
                    "keyword_translation_map": keyword_en_map,
                    "feedback_stats": stats,
                    "can_finetune": can_finetune,
                    "finetune_threshold": AUTO_FINETUNE_MIN_SAMPLES,
                    "db_added": db_result,
                    "db_error": db_error,
                }
            )

        # 2-1) ë¶€ëª¨ ë¼ë²¨ ê¸°ì¤€ custom_data ì €ì¥ (ê¸°ì¡´ parent head finetuneìš©)
        os.makedirs(CUSTOM_DATA_DIR, exist_ok=True)
        label_dir = os.path.join(CUSTOM_DATA_DIR, mapped_label)
        os.makedirs(label_dir, exist_ok=True)

        existing = [
            f for f in os.listdir(label_dir)
            if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp"))
        ]
        fname = f"{mapped_label}_{len(existing)}.jpg"
        saved_path = os.path.join(label_dir, fname)
        img.save(saved_path)

        # 2-2) synonym ì ë¦½ (ë¶€ëª¨ ë¼ë²¨ ê¸°ì¤€)
        english_synonyms = [en for en in keyword_en_map.values() if en.strip()]
        add_synonyms_for_label(mapped_label, english_synonyms)

        is_known = mapped_label in class_to_idx

        # 2-3) ì„œë¸Œë¼ë²¨ (main_keyword) ê¸°ì¤€ìœ¼ë¡œ sub_data ì €ì¥ + subclass_meta ì—…ë°ì´íŠ¸
        main_en = keyword_en_map.get(main_keyword, main_keyword)
        sub_id, sub_disp = extract_subclass_id(main_en)

        register_subclass_label(mapped_label, sub_id, sub_disp)

        os.makedirs(SUB_DATA_DIR, exist_ok=True)
        parent_dir = os.path.join(SUB_DATA_DIR, mapped_label)
        child_dir = os.path.join(parent_dir, sub_id)
        os.makedirs(child_dir, exist_ok=True)

        existing_sub = [
            f for f in os.listdir(child_dir)
            if f.lower().endswith((".jpg", ".jpeg", ".png", ".bmp"))
        ]
        sub_fname = f"{sub_id}_{len(existing_sub)}.jpg"
        sub_save_path = os.path.join(child_dir, sub_fname)
        img.save(sub_save_path)

        sub_label_id = sub_id
        sub_label_display = sub_disp

    stats = count_feedback_samples()
    can_finetune = stats["total"] >= AUTO_FINETUNE_MIN_SAMPLES

    return JSONResponse(
        {
            "message": "í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ",
            "saved_path": saved_path,
            "mapped_label": mapped_label,          # ë¶€ëª¨ Food-101 ë¼ë²¨
            "sub_label_id": sub_label_id,          # ì„œë¸Œë¼ë²¨ id (ì˜ˆ: green_onion_chicken)
            "sub_label_display": sub_label_display,# ì„œë¸Œë¼ë²¨ í‘œì‹œìš© í…ìŠ¤íŠ¸
            "is_known_class": is_known,
            "keyword_translation_map": keyword_en_map,
            "feedback_stats": stats,
            "can_finetune": can_finetune,
            "finetune_threshold": AUTO_FINETUNE_MIN_SAMPLES,
            "db_added": db_result,
            "db_error": db_error,
        }
    )

@app.post("/auto_collect_subdata_not_used_yet")
async def auto_collect_subdata(req: AutoCollectRequest):
    """
    food_dbì— ìˆëŠ” ìŒì‹ ì´ë¦„ë“¤ì„ ìë™ìœ¼ë¡œ ìˆœíšŒí•˜ë©´ì„œ:
    1) ìŒì‹ ì´ë¦„ìœ¼ë¡œ ì¸í„°ë„·ì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰
    2) ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œ
    3) EfficientNet(Food-101)ë¡œ ë¶€ëª¨ ë¼ë²¨ ì˜ˆì¸¡
    4) parent_label ë°‘ì— child_id(= food_db ì´ë¦„ ê¸°ë°˜) ì„œë¸Œë¼ë²¨ë¡œ ë“±ë¡
    5) ì´ë¯¸ì§€ë¥¼ sub_data/<parent>/<child>/ ì— ì €ì¥

    - í•œ ë²ˆ í˜¸ì¶œí•  ë•Œ max_items ê°œê¹Œì§€ë§Œ ì²˜ë¦¬
    - í´ë¼ì´ì–¸íŠ¸ê°€ start_indexë¥¼ ë°”ê¿”ê°€ë©° ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ë©´,
      ì‚¬ì‹¤ìƒ food_db ì „ì²´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ.
    """
    # 0) food_db ì „ì²´ ë¡œë“œ (ìš©ëŸ‰ì´ ë„ˆë¬´ í¬ë©´ ë‚˜ì¤‘ì— chunksizeë¡œ ë°”ê¿€ ìˆ˜ ìˆìŒ)
    df = load_food_db_df(limit=None)
    total = len(df)
    if total == 0:
        return JSONResponse(
            {"message": "food_dbì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "processed": 0},
            status_code=400,
        )

    start = max(0, req.start_index)
    end = min(total, start + req.max_items)
    if start >= total:
        return JSONResponse(
            {
                "message": "start_indexê°€ food_db ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.",
                "total": total,
                "processed": 0,
            },
            status_code=400,
        )

    processed_items = []

    for idx in range(start, end):
        row = df.iloc[idx]
        food_name = str(row["name"]).strip()
        if not food_name:
            continue

        # ê²€ìƒ‰ìš© ì¿¼ë¦¬ êµ¬ì„± (ê°„ë‹¨íˆ ìŒì‹ ì´ë¦„ + food)
        query = f"{food_name} food"
        print(f"[AUTO] ({idx}) '{food_name}' -> query='{query}'")

        urls = search_food_images_online(query, count=req.images_per_food)
        if not urls:
            print(f"[AUTO] ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {food_name}")
            continue

        child_display = food_name
        child_id = slugify_label(food_name)

        parent_labels_used = set()
        saved_count = 0

        for url in urls:
            img = download_image_from_url(url)
            if img is None:
                continue

            parent_label = predict_parent_label_from_pil(img)
            if parent_label is None:
                continue

            parent_labels_used.add(parent_label)

            # subclass_metaì— ë“±ë¡
            register_subclass_label(parent_label, child_id, child_display)

            # ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥
            path = save_sub_image(img, parent_label, child_id)
            saved_count += 1
            print(f"[AUTO] saved: parent={parent_label}, child={child_id}, path={path}")

        if saved_count > 0:
            processed_items.append(
                {
                    "index": idx,
                    "food_name": food_name,
                    "images_saved": saved_count,
                    "parent_labels": list(parent_labels_used),
                }
            )

    next_start = end if end < total else None

    return JSONResponse(
        {
            "message": "ìë™ ìˆ˜ì§‘ ì™„ë£Œ",
            "total": total,
            "start_index": start,
            "end_index": end,
            "processed": len(processed_items),
            "items": processed_items,
            "next_start_index": next_start,
        }
    )

# ---------------- API: í”¼ë“œë°± í†µê³„ ----------------

@app.get("/feedback_stats")
async def feedback_stats():
    """
    custom_data/ ì•„ë˜ì— ìŒ“ì¸ í”¼ë“œë°± ì´ë¯¸ì§€ ê°œìˆ˜ë¥¼ í™•ì¸.
    """
    stats = count_feedback_samples()
    can_finetune = stats["total"] >= AUTO_FINETUNE_MIN_SAMPLES
    return JSONResponse({
        "total": stats["total"],
        "labels": stats["labels"],
        "can_finetune": can_finetune,
        "finetune_threshold": AUTO_FINETUNE_MIN_SAMPLES,
    })

# ---------------- API: finetuneìš© ë°ì´í„° ê´€ë¦¬ (íƒ­ìš©) ----------------

@app.get("/finetune_data")
async def list_finetune_data():
    """
    finetune delete íƒ­ì—ì„œ ì‚¬ìš©í•  ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸.
    - items: [{id, label, filename, path}, ...]
    - ì´ idë¥¼ ê·¸ëŒ€ë¡œ DELETE /finetune_data/item/{id} ì— ì“°ë©´ ë¨.
    """
    items = list_finetune_items()
    return JSONResponse({"total": len(items), "items": items})


@app.delete("/finetune_data/item/{item_id}")
async def delete_finetune_item_by_id(item_id: int):
    """
    GET /finetune_data ë¡œ ë°›ì€ items ì¤‘ì—ì„œ íŠ¹ì • idë¥¼ ê°€ì§„ ì´ë¯¸ì§€ë¥¼ ì‚­ì œ.
    - í”„ë¡ íŠ¸ì—ì„œëŠ” ë¦¬ìŠ¤íŠ¸ì—ì„œ í´ë¦­í•œ í•­ëª©ì˜ idë¥¼ ê·¸ëŒ€ë¡œ ë„˜ê¸°ë©´ ë¨.
    """
    items = list_finetune_items()
    if item_id < 0 or item_id >= len(items):
        return JSONResponse({"error": "í•´ë‹¹ idì˜ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤."}, status_code=404)

    item = items[item_id]
    path = item["path"]
    label = item["label"]
    filename = item["filename"]

    if not os.path.exists(path):
        return JSONResponse({"error": "íŒŒì¼ì´ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}, status_code=404)

    try:
        os.remove(path)
        # í´ë”ê°€ ë¹„ì—ˆìœ¼ë©´ ì‚­ì œ
        label_dir = os.path.dirname(path)
        if os.path.isdir(label_dir) and len(os.listdir(label_dir)) == 0:
            os.rmdir(label_dir)
    except Exception as e:
        return JSONResponse({"error": f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}"}, status_code=500)

    return JSONResponse({"message": "íŒŒì¼ ì‚­ì œ ì™„ë£Œ", "id": item_id, "label": label, "filename": filename})


@app.delete("/finetune_data/label")
async def delete_finetune_label(label: str):
    """
    íŠ¹ì • ë¼ë²¨ì˜ ëª¨ë“  finetune ì´ë¯¸ì§€ë¥¼ ì‚­ì œ.
    (ì´ê±´ 'ì´ ë¼ë²¨ ì „ì²´ ì‚­ì œ' ë²„íŠ¼ìš©)
    """
    label_dir = os.path.join(CUSTOM_DATA_DIR, label)
    if not os.path.exists(label_dir):
        return JSONResponse({"error": "í•´ë‹¹ ë¼ë²¨ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}, status_code=404)

    removed_files = []
    try:
        for f in os.listdir(label_dir):
            path = os.path.join(label_dir, f)
            if os.path.isfile(path):
                os.remove(path)
                removed_files.append(f)
        try:
            os.rmdir(label_dir)
        except OSError:
            pass
    except Exception as e:
        return JSONResponse({"error": f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}"}, status_code=500)

    return JSONResponse({"message": "ë¼ë²¨ ë°ì´í„° ì‚­ì œ ì™„ë£Œ", "label": label, "removed": removed_files})

# ---------------- API: í—¤ë“œ íŒŒì¸íŠœë‹ (101 ë¶€ëª¨ìš©) ----------------

@app.post("/finetune_head")
async def finetune_head():
    """
    custom_data/ ì— ìŒ“ì¸ ì‚¬ìš©ì ë¼ë²¨ ë°ì´í„°ë¥¼ ê°€ì§€ê³ 
    EfficientNet í—¤ë“œ(classifier.1)ë§Œ ë¯¸ì„¸ì¡°ì •.
    """
    global model

    dataset = CustomLabeledDataset(CUSTOM_DATA_DIR, transform, class_to_idx)
    if len(dataset) == 0:
        return JSONResponse({"message": "custom_data ì— í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}, status_code=400)

    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

    for p in model.features.parameters():
        p.requires_grad = False
    for p in model.classifier.parameters():
        p.requires_grad = True

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=FINETUNE_LR)

    model.train()
    for epoch in range(FINETUNE_EPOCHS):
        running_loss = 0.0
        correct = total = 0
        for imgs, labels in dataloader:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

        avg_loss = running_loss / len(dataloader)
        acc = correct / total if total > 0 else 0.0
        print(f"[FINETUNE HEAD] Epoch {epoch+1}/{FINETUNE_EPOCHS} "
              f"Loss: {avg_loss:.4f}, Acc: {acc:.4f}")

    torch.save(model.state_dict(), MODEL_PATH)
    model.eval()
    return JSONResponse(
        {"message": "ê¸€ë¡œë²Œ í—¤ë“œ íŒŒì¸íŠœë‹ ì™„ë£Œ", "epochs": FINETUNE_EPOCHS}
    )

# ---------------- API: ë¶€ëª¨ë³„ ì„œë¸Œí—¤ë“œ íŒŒì¸íŠœë‹ ----------------

@app.post("/finetune_subheads")
async def finetune_subheads():
    """
    sub_data/<parent>/<child>/*.jpg ë¥¼ ì‚¬ìš©í•´ì„œ
    ë¶€ëª¨ë³„ feature-head(Linear(1280, n_child))ë¥¼ í•™ìŠµí•œë‹¤.
    """
    meta = load_subclass_meta()
    if not meta:
        return JSONResponse({"message": "ì„œë¸Œí´ë˜ìŠ¤ ë©”íƒ€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. feedbackìœ¼ë¡œ ë¨¼ì € ì„œë¸Œë¼ë²¨ì„ ì¶”ê°€í•˜ì„¸ìš”."}, status_code=400)

    os.makedirs(SUBHEAD_DIR, exist_ok=True)
    train_results = []

    for parent_label, info in meta.items():
        children = info.get("children", [])
        child_ids = [c["id"] for c in children]
        if len(child_ids) < 2:
            print(f"[INFO] parent={parent_label} ëŠ” ì„œë¸Œí´ë˜ìŠ¤ê°€ 2ê°œ ë¯¸ë§Œì´ë¼ ìŠ¤í‚µ")
            continue

        dataset = SubclassDataset(parent_label, child_ids, transform)
        if len(dataset) < len(child_ids) * 2:  # ë„ˆë¬´ ë°ì´í„°ê°€ ì ìœ¼ë©´ ìŠ¤í‚µ
            print(f"[INFO] parent={parent_label} ì˜ ë°ì´í„°({len(dataset)})ê°€ ë„ˆë¬´ ì ì–´ ìŠ¤í‚µ")
            continue

        dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)
        head = nn.Linear(1280, len(child_ids)).to(DEVICE)
        optimizer = torch.optim.Adam(head.parameters(), lr=SUBHEAD_LR)
        criterion = nn.CrossEntropyLoss()

        head.train()
        for ep in range(SUBHEAD_EPOCHS):
            running_loss = 0.0
            correct = total = 0
            for imgs, ys in dataloader:
                imgs = imgs.to(DEVICE)
                ys = ys.to(DEVICE)

                feats = extract_features(imgs)  # [B,1280], grad X (featsëŠ” ìƒìˆ˜)
                optimizer.zero_grad()
                logits = head(feats)
                loss = criterion(logits, ys)
                loss.backward()
                optimizer.step()

                running_loss += loss.item()
                _, preds = torch.max(logits, 1)
                correct += (preds == ys).sum().item()
                total += ys.size(0)

            avg_loss = running_loss / len(dataloader)
            acc = correct / total if total > 0 else 0.0
            print(f"[SUBHEAD] parent={parent_label} Epoch {ep+1}/{SUBHEAD_EPOCHS} "
                  f"Loss: {avg_loss:.4f}, Acc: {acc:.4f}")

        head_path = os.path.join(SUBHEAD_DIR, f"{parent_label}_head.pth")
        torch.save(head.state_dict(), head_path)
        if parent_label in _subhead_cache:
            del _subhead_cache[parent_label]

        train_results.append({
            "parent_label": parent_label,
            "children": child_ids,
            "samples": len(dataset),
        })

    if not train_results:
        return JSONResponse({"message": "í•™ìŠµí•  ì„œë¸Œí—¤ë“œê°€ ì—†ê±°ë‚˜, ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤."}, status_code=400)

    return JSONResponse({"message": "ì„œë¸Œí—¤ë“œ íŒŒì¸íŠœë‹ ì™„ë£Œ", "results": train_results})

# ---------------- API: food_db ì¡°íšŒ ----------------

@app.get("/foods")
async def get_foods(limit: int = 100, offset: int = 0):
    """
    food_dbì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ëœ ê²°ê³¼ë¥¼ ë°˜í™˜.
    """
    if limit <= 0:
        limit = 50

    df = load_food_db_df(limit=limit, offset=offset)
    foods = df.to_dict(orient="records")

    return JSONResponse({
        "foods": foods,
        "limit": limit,
        "offset": offset,
        "count": len(foods),
    })


@app.get("/foods/names")
async def get_food_names(limit: int = 200, offset: int = 0):
    """
    food_dbì— ì €ì¥ëœ ìŒì‹ ì´ë¦„ë§Œ ë¶€ë¶„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    """
    df = load_food_db_df(limit=limit, offset=offset)
    names = df["name"].dropna().astype(str).tolist()
    return JSONResponse({
        "names": names,
        "limit": limit,
        "offset": offset,
        "count": len(names),
    })

# ---------------- API: í…ìŠ¤íŠ¸ë¡œ ìŒì‹ ê²€ìƒ‰ (í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ í¬í•¨) ----------------

@app.post("/foods/search")
async def search_foods(req: FoodSearchRequest):
    """
    ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•œ ìŒì‹ ì´ë¦„(í•œê¸€/ì˜ì–´)ì„ ê¸°ë°˜ìœ¼ë¡œ
    food_dbì—ì„œ ë¹„ìŠ·í•œ ìŒì‹ë“¤ ìƒìœ„ topkë¥¼ ë°˜í™˜.
    """
    q_raw = req.query.strip()
    if not q_raw:
        return JSONResponse({"error": "query is empty"}, status_code=400)

    load_text_matcher()
    q_en, translated = translate_to_en_if_korean(q_raw)
    candidates = search_food_candidates_masked(q_en, keywords=[], topk=req.topk)

    return JSONResponse({
        "query": q_raw,
        "used_query": q_en,
        "translated": translated,
        "candidates": candidates,
    })

# ---------------- API: ìƒˆ ìŒì‹ ì¶”ê°€ ----------------

@app.post("/foods/add")
async def add_food(food: FoodCreate):
    """
    ìƒˆ ìŒì‹ + íƒ„/ë‹¨/ì§€(í•„ìˆ˜) + ì„ íƒì ì¸ ì¹¼ë¡œë¦¬, ì„œë¹™ ë‹¨ìœ„ ë“±ì„
    food_db.csvì— ì¶”ê°€í•œë‹¤.
    - nameì— í•œê¸€ì´ ìˆìœ¼ë©´ ì˜ì–´ë¡œ ë²ˆì—­í•´ì„œ nameìœ¼ë¡œ ì €ì¥.
    """
    try:
        new_row, meta = add_food_row(
            name=food.name,
            carbs=food.carbs,
            protein=food.protein,
            fat=food.fat,
            serving_size=food.serving_size,
            unit=food.unit,
            calories=food.calories,
        )
        # âœ… ìƒˆ ìŒì‹ì´ ì¶”ê°€ëìœ¼ë‹ˆ í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ë„ ìµœì‹ ìœ¼ë¡œ ê°±ì‹ 
        load_text_matcher()              # vectorizer ë¡œë“œê°€ ì•ˆ ë˜ì–´ ìˆë‹¤ë©´ ë¨¼ì € ë¡œë“œ
        rebuild_text_index_from_food_db()
    except ValueError as ve:
        return JSONResponse({"error": str(ve)}, status_code=400)
    except Exception as e:
        return JSONResponse({"error": f"DB ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}"}, status_code=500)

    return JSONResponse(
        {
            "message": "ìƒˆ ìŒì‹ì´ food_dbì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "food": new_row,
            "meta": meta,
        }
    )

# ---------------- main: python server.py ë¡œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥ ----------------

if __name__ == "__main__":
    import uvicorn
    print("[INFO] ì„œë²„ ì‹œì‘: http://0.0.0.0:8000  (docs: /docs)")
    uvicorn.run(app, host="0.0.0.0", port=8000)
